{
  "hash": "269ffdc59b075e981e318c1054cadb71",
  "result": {
    "markdown": "---\ntitle: Generative vs. discriminative models\nsubtitle: Thinking through the differences between generative and discriminative models.\ndate: 2022/04/03\nimage: graph.png\ncategories:\n  - factor graphs\n  - PGMs\n  - ML\n  - theory\necho: false\n---\n\n\n\n\n\n\n\nLooking around, there seem to be multiple definitions of what discriminative and generative models actually are, ranging from too narrow to outright confusing, as well as lots of claims about advantages and disadvantages of one over the other, but often with little detail or justification. \nHowever, if one ignores the miriad references on this topic and just reads through @bishop2006pattern [Chapter 4] and @minka2005discriminative, one gets a fairly good picture of what's what.\n\nWe will work through the definitions and differences by considering _probabilistic (graphical) models_, specifically using the _factor graph_ representation[^1].\nThroughout, we will focus on _classification_, where we are given a labelled data set $D = \\lbrace \\left( x^i, y^i \\right) \\rbrace_N$ with inputs $x^i = \\left( x^i_1, \\cdots, x^i_n \\right)$ and labels $y^i \\in \\lbrace 0, 1 \\rbrace$, and we want $P \\left( y = 1 \\vert x = \\hat x \\right)$ for some (new) $\\hat x$[^2].\nWe should keep in mind that the concepts and the generative-discriminative dichotomy are more general, i.e. they are not restricted to the problem of classification.\n\n[^1]: Factor graphs are a more detailed representation of a probabilistic graphical model that specifies the factorization of the joint probability being modelled.\nSee e.g. @loeliger2004introduction.\n\n[^2]: Note that actually determining which class to assign to $\\hat x$ requires a final decision. \nWe will ignore this and _discriminant functions_, which combine the inference and the decision steps.\nSee [@bishop2006pattern, Section 1.5.4] for a discussion.\n\n## Generative models can be used to generate synthetic data\n\nMany references jump straight in and define deterministic and generative models, in the context of classification, via the probabilities that they consider. \nA _discriminative model_ is defined as one that provides $P \\left( y \\vert x \\right)$ directly.\nThe name comes from the fact that we can view the distribution (together with the decision to choose the most probable value) as directly discriminating the value of the target $y$ for any given instance $x$.\n\n::: {.cell execution_count=4}\n\n::: {.cell-output-display execution_count=3}\n![Discriminative model, as used for prediction.](index_files/figure-html/cell-5-output-1.svg)\n:::\n:::\n\n\nMirorring the discriminative model definition, a _generative model_ is instead defined as one that provides the joint probability $P \\left( x, y \\right)$ in the form $P \\left( x \\vert y \\right)P \\left( y \\right)$, on which one can then use Bayes' theorem to obtain $P \\left( y \\vert x \\right) \\propto P \\left( x \\vert y \\right)P \\left( y \\right).$ \nThe graph representation for such a model is shown below.\n\n::: {.cell execution_count=5}\n\n::: {.cell-output-display execution_count=4}\n![Generative model with \"default\" factorization, as used for prediction.](index_files/figure-html/cell-6-output-1.svg)\n:::\n:::\n\n\nThe more general definition is that a generative model is one that can generate data via _ancestral sampling_, i.e. sampling from the priors and passing the sampled values through the graphical model.\nThis includes the case above, but we could also turn our discriminative model into a generative one by adding a factor for $P \\left( x \\right)$, thus turning it into a model of the joint distribution $P \\left( x, y \\right)$. \nPut differently, the factorization used in the definition of the generative model above is not what makes it a generative model. \nIt is rather the fact that it models the joint distribution.\nThat being said, if one is building a model for prediction, or to infer causal impact of $x$ on $y$, and observes $x$ ($= \\hat x$) then adding a prior on $x$ to the discriminative model is generally not of much use.\nIn order to distinguish between the (first) generative model and the extended discriminative model, which is also a generative model, Mitchell refers to the former as a _Bayes classifier_ given that it uses Bayes theorem to recover $P \\left( y \\vert x \\right)$ [@mitchell2020generative].\n\n::: {.cell execution_count=6}\n\n::: {.cell-output-display execution_count=5}\n![Extended discriminative model with prior for $x$, as used for prediction.](index_files/figure-html/fg-discr-extended-output-1.svg){#fg-discr-extended}\n:::\n:::\n\n\nNote that we sometimes also find people stating that generative models are ones that capture the causal process by which the actual data ($D$) is generated. \nDespite devoting a lot of pages to the discriminative and generative models, @bishop2006pattern also uses this definition in Section 8.1.2, albeit in passing.\nSimilarly, it is not uncommon to hear people refer to the models considered in McElreath's Statistical Rethinking [-@mcelreath2020statistical] as generative models, given all the space devoted to the causal DAGs and data generation, even though the models are mostly discriminative regressions. \nIndeed, while it is true that one might build a generative model by thinking about the causal process, it could be that the causal _data generation process_ requires $P \\left( y \\vert x \\right)$ rather than $P \\left( x \\vert y \\right)$. \nWe therefore distinguish between generative models and generative processes.\n\n## Generative models consider a more restictive parameter space\n\nLet's now look at the fundamental difference between the two model types by considering them in all generality and focusing on their parametrisation as done by @minka2005discriminative.\n\nWe write the generative model with parameters as\n$$\nP_1 \\left( x, y \\vert \\theta \\right) = P_{11} \\left( x \\vert y, \\theta \\right) P_{12} \\left( y \\vert \\theta \\right).\n$$\nWe can train the model, i.e. perform inference, to obtain the posterior probability $P \\left( \\theta \\vert D \\right)$ by considering the joint distribution\n$$\n\\begin{align*}\nP_g \\left( D, \\theta \\right)    &= P_{01} \\left( \\theta \\right) P_1 \\left( D \\vert \\theta \\right) \\\\\n                                &= P_{01} \\left( \\theta \\right) \\prod_i P_1 \\left( x_i, y_i \\vert \\theta \\right) \\\\\n                                &= P_{01} \\left( \\theta \\right) \\prod_i P_{11} \\left( x_i \\vert y_i, \\theta \\right) P_{12} \\left( y_i \\vert \\theta \\right),\n\\end{align*}\n$$\nwhere we have used the iid assumption on the data.\n\n::: {.cell execution_count=7}\n\n::: {.cell-output-display execution_count=6}\n![Parametrised generative model with training data plate.](index_files/figure-html/cell-8-output-1.svg)\n:::\n:::\n\n\nAlternatively, we can use maximum likelihood estimation to find $\\hat \\theta$. \nThe BIASlab couse nicely explains the different approaches with examples [@devries2021bmlip].\n\nNow let's write the discriminative model, with parameters, as $P_{21} \\left( y \\vert x, \\theta \\right)$. \nIn order to compare it with the generative model, we extend the discriminative model by adding a probability over $x$ and a second parameter in order to obtain the joint distribution\n$$\nP_2 \\left( x, y \\vert \\theta, \\phi \\right) = P_{21} \\left( y \\vert x, \\theta \\right) P_{22} \\left( x \\vert \\phi \\right),\n$$\nbut specifically consider the same joint distribution by setting\n$$\nP_{21} \\left( y \\vert x, \\theta \\right) = \\frac{P_1 \\left( x, y \\vert \\theta \\right)}{\\sum_y P_1 \\left( x, y \\vert \\theta \\right)}\n$$\nand\n$$\nP_{22} \\left( x \\vert \\phi \\right) = \\sum_y P_1 \\left( x, y \\vert \\phi \\right).\n$$\n\nThe parameters $\\theta$ and $\\phi$ are of the same type, but assumed independent. \nWe can again obtain the posterior distributions for the parameters by considering the joint distribution\n$$\n\\begin{align*}\nP_d \\left( D, \\theta, \\phi \\right)    &= P_{01} \\left( \\theta \\right) P_{02} \\left( \\phi \\right) P_2 \\left( D \\vert \\theta, \\phi \\right) \\\\\n                                &= P_{01} \\left( \\theta \\right) P_{02} \\left( \\phi \\right) \\prod_i P_2 \\left( x_i, y_i \\vert \\theta, \\phi \\right) \\\\\n                                &= P_{01} \\left( \\theta \\right) P_{02} \\left( \\phi \\right) \\prod_i P_{21} \\left( y_i \\vert x_i, \\theta \\right) P_{22} \\left( x_i \\vert \\phi \\right) \\\\\n                                &= \\left( P_{01} \\left( \\theta \\right) \\prod_i P_{21} \\left( y_i \\vert x_i, \\theta \\right) \\right) \\left(P_{02} \\left( \\phi \\right) \\prod_i P_{22} \\left( x_i \\vert \\phi \\right) \\right),\n\\end{align*}\n$$\nand inferring $P \\left( \\theta, \\phi \\vert D \\right)$.\n\n::: {.cell execution_count=8}\n\n::: {.cell-output-display execution_count=7}\n![Parametrised, extended discriminative model with training data plate.](index_files/figure-html/cell-9-output-1.svg)\n:::\n:::\n\n\nWe note that, due to the independence assumption, estimation of $\\theta$ and $\\phi$ decouples, namely if we use the factorization above to define\n$$\nP_d \\left( D, \\theta, \\phi \\right) =: P^1 \\left( y, \\theta \\vert x \\right) P^2 \\left( x, \\phi \\right),\n$$\nthen we see that Bayes' rule simplifies, that is\n$$\n\\begin{align*}\nP \\left( \\theta, \\phi \\vert D \\right)   &= \\frac{P_d \\left( D, \\theta, \\phi \\right)}{\\sum_{\\theta, \\phi} P_d \\left( D, \\theta, \\phi \\right)} \\\\\n                                        &= \\frac{P^1 \\left( y, \\theta \\vert x \\right) P^2 \\left( x, \\phi \\right)}{\\sum_{\\theta , \\phi} P^1 \\left( y, \\theta \\vert x \\right) P^2 \\left( x, \\phi \\right)} \\\\\n                                        &= \\frac{P^1 \\left( y, \\theta \\vert x \\right)}{\\sum_\\theta P^1 \\left( y, \\theta \\vert x \\right) } \\frac{P^2 \\left( x, \\phi \\right)}{\\sum_\\phi P^2 \\left( x, \\phi \\right)} \\\\ \n                                        &=: P \\left( \\theta \\vert D \\right) P \\left( \\phi \\vert x \\right).\n\\end{align*}\n$$\n\n::: {.cell .column-margin execution_count=9}\n\n::: {.cell-output-display}\n![Comparison of parameter space considered by models. The generative model only considers the hyperplane $\\theta = \\phi.$](index_files/figure-html/cell-10-output-1.png){width=211 height=142}\n:::\n:::\n\n\nThus $\\hat \\theta$ (or equivalently $P \\left( \\theta \\vert D \\right)$) is unaffected by the estimation of $\\hat \\phi$ and is the same as what we would have obtained by performing inference on the original, non-extended discriminative model.\n\nWe see that the fundamental difference between the two models is down to the discriminative one considering a larger parameter space without the constraint $\\theta = \\phi$.\nThis reduces the (statistical) bias, but introduces variance.\n\nWe have just shown that the generative model is more biased, but this is a more fundamental difference than what is usually discussed. Indeed, the joint distribution considered by the generative model is often hard to work with in practice and therefore further simplifying assumptions are often necessary, or preferable, in order to make inference tractable. This can lead to further bias of the generative model, which we will consider next.\n\n## Generative models require more assumptions\n\nTo understand why generative models require more modelling assumptions in order to make inference manageable, we will consider the case of Boolean inputs $x = \\left( x_1, \\cdots, x_n \\right)$, $x_j \\in \\lbrace 0, 1 \\rbrace$.\n\nIt can be instructive to update the factor graphs and draw some of the individual components of the input.\n\n::: {.cell execution_count=10}\n\n::: {.cell-output-display execution_count=9}\n![Discriminative model with two components of the input vector drawn.](index_files/figure-html/cell-11-output-1.svg)\n:::\n:::\n\n\n::: {.cell execution_count=11}\n\n::: {.cell-output-display execution_count=10}\n![Generative model with two components of the input vector drawn.](index_files/figure-html/cell-12-output-1.svg)\n:::\n:::\n\n\nLet us now look at the parameters necessary for the generative model by first considering the conditional probability table for $P\\left( x \\vert y \\right)$ with $x$ represented as a single vector.\n\n|                      |                 |                   |\n|----------------------|-----------------|-------------------|\n|                      | $y = 0$         | $y = 1$           |\n| $x = (0, \\cdots, 0)$ | $\\theta^0_{1}$  | $\\theta^1_{1}$    |\n| $x = (1, \\cdots, 0)$ | $\\theta^0_{2}$  | $\\theta^1_{2}$    |\n| $\\cdots$             | $\\cdots$        | $\\cdots$          |\n| $x = (1, \\cdots, 1)$ | $\\theta^0_{2^n}$ | $\\theta^1_{2^n}$   |\n\nWe see that we have $2 \\times 2^n = 2^{n + 1}$ parameters. \nThe (conditional) probability constraints (on the columns) bring this count down to $2 \\left( 2^n - 1\\right)$.\n\nThe other factor in the generative model, $P \\left( y \\right)$, is not an issue, as we only have one effective parameter given $y$ is a Boolean variable.\n\nFor the discriminative model, we instead have to consider $P \\left( y \\vert x \\right)$.\nHere the conditional probability table is flipped.\n\n|         |                      |          |                      |\n|---------|----------------------|----------|----------------------|\n|         | $x = (0, \\cdots, 0)$ | $\\cdots$ | $x = (1, \\cdots, 1)$ |\n| $y = 0$ | $\\theta^0_{1}$       | $\\cdots$ | $\\theta^0_{2^n}$      |\n| $y = 1$ | $\\theta^1_{1}$       | $\\cdots$ | $\\theta^1_{2^n}$      |\n\nSimply flipping the conditionality leads to $2^n$ effective parameters, again using the conditional probability constraints. \nThis is less parameters than those for the generative model. \nFor large $n$, essentially half as many.\n\nWhat is often done in generative models is to add further simplifying assumptions.\nIn the Naive Bayes classifier for example, we assume each $x_i$ is conditionally independent of all other $x_{-i}$ given $y$.\nTogether with the product rule, this gives\n$$\nP \\left(x \\vert y \\right) = \\prod_i P \\left( x_i \\vert y \\right).\n$$\n\nWe can visualise this more granular factorization of the conditional probability by drawing the factor graph. \nThis time using plate notation.\n\n::: {.cell execution_count=12}\n\n::: {.cell-output-display execution_count=11}\n![Generative model with Naive Bayes assumption.](index_files/figure-html/cell-13-output-1.svg)\n:::\n:::\n\n\nNow, each $x_i$ has its own conditional probability table, which is simply\n\n|           |                 |                   |\n|-----------|-----------------|-------------------|\n|           | $y = 0$         | $y = 1$           |\n| $x_i = 0$ | $\\theta^0_{0}$  | $\\theta^1_{0}$    |\n| $x_i = 1$ | $\\theta^0_{1}$  | $\\theta^1_{1}$    |\n\nand the conditional probability constraints bring the number of parameters per input variable from four to two. \nThus, overall we have $2 n$ parameters to estimate.\nThis is now less than the $2^n$ of the discriminative model (provided $n > 2$).\n\nOn top of the number of parameters to estimate, in order to reliably estimate them, we need to observe each distinct instance multiple times.\nThis is discussed in [@mitchell2020generative].\n\nWe thus can, and often do, introduce futher bias in generative models in order to make them tractable.\nA consequence of this is that generative models can be less accurate, if they (i.e. the small world model) don't reflect the large world model[^3], but (when they do) generative models require less data to train.\n\n[^3]: Borrowing Savage's [vocabulary](https://errorstatistics.files.wordpress.com/2021/03/savage-forum-combined-searchable_red.pdf), as presented by McElreath in Statistical Rethinking: _\"All statisitcal modelling has these two frames: the small world of the model itself and the large world we hope to deploy the model in.\"_ [@mcelreath2020statistical, page 19].\n\n## Generative models can deal with missing data\n\nLet's turn to the fact, which is often mentioned, that generative models can deal with missing data.\nWhat this means is that they can still make predictions if given a vector of inputs $\\hat x = \\left( \\hat x_1, \\cdots, \\hat x_k, \\bar x_{k+1}, \\cdots, \\bar x_n \\right) = \\left( \\hat x_o, \\bar x_m \\right)$, where $\\bar x_m$ are missing, whereas discriminative models can't.\n\nWhen it comes to predicting $\\hat y$ given $\\hat x$, we need the posterior predictive distribution, namely\n$$\n\\begin{align*}\nP \\left( \\hat y \\vert \\hat x, D \\right) &= \\int_\\Theta P \\left( \\hat y \\vert \\hat x, D, \\theta \\right) P \\left( \\theta \\vert D \\right) \\mathrm{d} \\theta \\\\\n                                        &= \\int_\\Theta P \\left( \\hat y \\vert \\hat x, \\theta \\right) P \\left( \\theta \\vert D \\right) \\mathrm{d} \\theta\n\\end{align*}\n$$\nwhere we assume that the past and future observations are conditionally independent given $\\theta$.\n\nIn the case of missing inputs, we want to consider\n$$\n\\begin{align*}\nP \\left( \\hat y \\vert \\hat x_o, \\theta \\right)  &= \\sum_{\\bar x_m} P \\left( \\hat y, \\bar x_m \\vert \\hat x_o, \\theta \\right) \\\\\n                                                &= \\sum_{\\bar x_m} P \\left( \\hat y \\vert \\hat x_o, \\bar x_m , \\theta \\right) P \\left( \\bar x_m \\vert \\hat x_o, \\theta \\right)\n\\end{align*}\n$$\nand plug this into the posterior predictive distribution.\n\n::: {.cell execution_count=13}\n\n::: {.cell-output-display execution_count=12}\n![Discriminative model being used for prediction, with missing inputs $\\bar x_m$. Note that $Xo$, $Xm$ and $Y$ replace $\\hat x_o$, $\\bar x_m$ and $\\hat y$ due to graphviz limitaitons.](index_files/figure-html/cell-14-output-1.svg)\n:::\n:::\n\n\nIn the case of discriminative models, we have no way of evaluating the necessary probabilities because we only have $P_{21} \\left( y \\vert x, \\theta \\right)$. \nWe therefore cannot obtain $P \\left( \\bar x_m \\vert \\hat x_o, \\theta \\right)$.\nWe would need to instead resort to some form of imputation. \nThis equates to making assumptions about the distribution $P \\left( x \\right)$, which we would instead have if we consider an extended discriminative model.\nThese can indeed deal with missing observations, given they model the full joint distribution, explicitly via $P \\left( y \\vert x \\right)$ and $P \\left(x \\right)$.\n\n::: {.cell execution_count=14}\n\n::: {.cell-output-display execution_count=13}\n![Generative model being used for prediction, with missing inputs $\\bar x_m$. Note that $Xo$, $Xm$ and $Y$ replace $\\hat x_o$, $\\bar x_m$ and $\\hat y$ due to graphviz limitaitons.](index_files/figure-html/cell-15-output-1.svg)\n:::\n:::\n\n\nIn the generative case, we instead have the joint distribution $P_1 \\left( x, y \\vert \\theta \\right)$. \nWe can therefore use Bayes theorem to get $P \\left( \\hat y \\vert \\hat x_o, \\bar x_m , \\theta \\right)$, as we would anyhow for prediction, and then use the joint distribution with the necessary marginalisations to get $P \\left( \\bar x_m \\vert \\hat x_o, \\theta \\right)$.\n\nWe can also consider missing data more generally, including missing labels and missing inputs in the training data.\nIn the case of generative models, we can train them both in an unsupervised way, when we have no labels, and a semi-supervised way, when we have a few labels.\nIn the case of discriminative models, Minka points out that the extended model can also be trained in a semi-supervised fashion [-@minka2005discriminative].\nWe will cover this in a future post.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}